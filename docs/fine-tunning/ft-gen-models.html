
<!DOCTYPE html>


<html lang="zh-CN" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-GESLLLJC6M"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-GESLLLJC6M');
    </script>
    
    <title>微调生成式大模型 &#8212; 人工智能实践 0.6 文档</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=be9b6ff4" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../_static/documentation_options.js?v=ed3f5f1b"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/translations.js?v=beaddf03"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'fine-tunning/ft-gen-models';</script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="微调表示型大模型" href="ft-rep-models.html" />
    <link rel="prev" title="提示词越狱" href="../6-Prompts/jail-break.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="zh-CN"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">人工智能实践 0.6 文档</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="搜索" aria-label="搜索" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">搜索</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">课程简介</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../1-Intro/intro.html">课程大纲</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1-Intro/resources.html">资源</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1-Intro/terms.html">术语</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">大模型基础</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../2-Basics/setup.html">环境准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2-Basics/llm-basics.html">LLM 基础</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">基于Transformer的自然语言处理</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../3-Practice/nlp-tasks.html">NLP 任务</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3-Practice/transformer-nlp.html">基于HF Transformer的NLP实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3-Practice/classification.html">文本分类</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3-Practice/clusttering.html">文档聚类</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">语言学基础</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../4-Linguistics/linguistics-llm.html">大语言模型与语言学</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4-Linguistics/llm-metrics.html">常见大模型评价指标</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4-Linguistics/translation-metrics.html">译文质量评价方法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4-Linguistics/linguistics-intro.html">语言学简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4-Linguistics/applied-linguistics.html">应用语言学</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">课程实践项目</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../5-Projects/projects.html">课程实践项目要求</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5-Projects/evaluating-llms.html">大模型的评估与选择</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5-Projects/evaluate-case.html">评价模型翻译能力</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">提示词</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../6-Prompts/prompts-intro.html">提示词导论</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6-Prompts/prompts-examples.html">提示词案例分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6-Prompts/langchain.html">基于Langchain的提示词开发实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6-Prompts/good-examples.html">提示词案例分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6-Prompts/jail-break.html">提示词越狱</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">微调</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">微调生成式大模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="ft-rep-models.html">微调表示型大模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlm-bert.html">使用MLM微调Bert模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="ft-phi4.html">SFT 微调 Phi-4</a></li>
<li class="toctree-l1"><a class="reference internal" href="ft-ph4-self.html">练习：使用自监督学习微调Phi4-mini</a></li>
<li class="toctree-l1"><a class="reference internal" href="ft-rl.html">基于人类反馈微调大模型（RLHF + DPO）</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">参考答案</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../answers/transformer-practice-answers.html">基于HF Transformer的NLP实践的练习答案</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="下载此页面">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/fine-tunning/ft-gen-models.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="下载源文件"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="列印成 PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="全屏模式"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="搜索" aria-label="搜索" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>微调生成式大模型</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> 目录 </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">预训练</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sft">有监督微调（SFT）</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">偏好微调</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#peft">参数高效微调（PEFT）</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adapters">Adapters（适配器）</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4"></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lora">低秩适配（LoRA）</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">压缩模型以实现（更）高效的训练</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rlhf">偏好调优 / 对齐 / 基于强化学习的人类反馈（RLHF）</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">使用奖励模型自动化偏好评估</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">奖励模型的输入与输出</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">训练奖励模型</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">奖励模型训练数据集</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">奖励模型训练步骤</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">训练不使用奖励模型</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="id1">
<h1>微调生成式大模型<a class="headerlink" href="#id1" title="Link to this heading">#</a></h1>
<p>大语言模型的开发训练通常分三步：预训练、有监督微调和偏好微调。这三个训练步骤如下图所示。</p>
<p><img alt="three-steps" src="../_images/three-steps.png" /></p>
<p><em>从一个未训练的模型架构开始，最终得到经偏好微调的语言模型的完整过程</em></p>
<section id="id2">
<h2>预训练<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>创建高质量大型语言模型（LLM）的第一步是使用海量文本数据集进行预训练。在训练过程中，通常使用自监督方法，让模型尝试预测下一个词元（token），以准确学习文本中的语言和语义表示，这种过程也称为语言建模。训练后会生成一个基础模型（base model），也常被称为预训练模型或基础模型。基础模型是训练过程中产生的一个关键成果，但对终端用户来说，它们往往难以直接使用。这也正是微调模型较为重要的原因。本节我们将讲解如何微调生成式的模型基本方法。</p>
</section>
<section id="sft">
<h2>有监督微调（SFT）<a class="headerlink" href="#sft" title="Link to this heading">#</a></h2>
<p>大语言模型需能很好地响应并遵循用户指令时，才会变得有用。例如，问 GPT-3基座模型模型：Tell me how to fine-tune a model. 模型的回答很可能是重复与问题类似的问题，如 How can I control the complexity of a model，而不是直接回答用户的问题。</p>
<p><img alt="ft" src="../_images/ft.png" /></p>
<p>通过<strong>有监督微调（SFT）</strong>，我们可以让基础模型学会遵循指令。在这个微调过程中，基础模型的参数会被更新，以更好地适应我们设定的目标任务，例如更准确地理解和执行指令。</p>
<p>和预训练模型一样，有监督微调也使用“下一个词元预测”（next-token prediction）的方法进行训练。但不同的是，它不是单纯地预测下一个词元，而是基于用户的输入来进行预测。</p>
</section>
<section id="id3">
<h2>偏好微调<a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<p>最后是进一步提升模型的输出质量，使其更加符合 AI 安全要求或人类偏好的预期行为。这一步称为<strong>偏好微调（preference tuning）</strong>。偏好微调是一种微调方法，顾名思义，它通过我们提供的数据，将模型的输出调整得更加贴合我们的偏好。</p>
<p>与有监督微调类似，偏好微调也可在基座模型上进行微调，其额外优势在于：它在训练过程中引入了对输出偏好的“提炼”过程。</p>
</section>
<section id="peft">
<h2>参数高效微调（PEFT）<a class="headerlink" href="#peft" title="Link to this heading">#</a></h2>
<p>更新模型的所有参数或可显著提升模型性能，但会有不少缺点，比如：训练成本高、训练速度慢、对存储空间需求大。为了应对这些问题，研究人员提出了参数高效微调（PEFT）的方法，这些方法可更高效地对预训练模型进行微调。</p>
<section id="adapters">
<h3>Adapters（适配器）<a class="headerlink" href="#adapters" title="Link to this heading">#</a></h3>
<p><strong>适配器</strong>是许多 PEFT 技术的核心组成部分。这种方法的核心是在 Transformer 结构中加入一组额外的模块化组件，并只对这些组件进行微调，从而提升模型在特定任务上的表现，而无需更新模型的全部权重，这大大节省了时间和计算资源。</p>
<p>适配器的概念最早出现在论文<a class="reference external" href="https://arxiv.org/abs/1902.00751">Parameter-efficient transfer learning for NLP</a>中，该研究发现：仅微调 BERT 模型 <strong>3.6% 的参数</strong>，就可获得与全参数微调接近的性能。在 GLUE 基准测试上，作者的结果与全参数微调的性能仅相差 <strong>0.4%</strong>。</p>
<p>在单个 Transformer 模块中，该论文提出的架构将适配器分别放置在 <strong>注意力层</strong> 和 <strong>前馈神经网络层</strong> 之后，如下图所示：</p>
<p><img alt="adapter" src="../_images/adapter.png" /></p>
<p>然而，仅仅修改单个 Transformer 模块还远远不够。这些适配器组件会加入到模型的<strong>每一个Transformer模块中</strong>，如下图 所示：</p>
<p><img alt="adapters" src="../_images/adapters.png" /></p>
<section id="id4">
<h4><a class="headerlink" href="#id4" title="Link to this heading">#</a></h4>
<p>我们也可以将整个模型中所有适配器组件都可视化出来，如下图所示。我们就能把每个适配器单独看作是一个模块的集合，分别分布在模型的所有层中。比如，<strong>适配器1</strong> 可以专门用于医疗文本分类，而 <strong>适配器2</strong> 则可以专注于命名实体识别（NER）等任务。</p>
<p><img alt="adapters-in-all-blocks" src="../_images/adapters-in-all-blocks.png" /></p>
<p>可以 <a class="reference external" href="https://adapterhub.ml">Adapter Hub</a> 网站下载各种专门用途的适配器模块。</p>
<p><img alt="adapter-hub" src="../_images/adapter-hub.png" /></p>
<blockquote>
<div><p>论文<a class="reference external" href="https://arxiv.org/abs/2007.07779">AdapterHub: A framework for adapting transformers</a> 提出了 <strong>AdapterHub</strong>这一个用于共享适配器的中央仓库。早期的许多适配器主要集中在 <strong>BERT 架构</strong> 上。</p>
<p>近年来，这一概念也被应用到了文本生成类的 Transformer 模型中，例如论文<a class="reference external" href="https://arxiv.org/abs/2303.16199">LLaMA-Adapter: Efficient fine-tuning of language models with zero-init attention</a>，展示了如何高效地对语言模型进行微调。</p>
</div></blockquote>
</section>
</section>
</section>
<section id="lora">
<h2>低秩适配（LoRA）<a class="headerlink" href="#lora" title="Link to this heading">#</a></h2>
<p>低秩适配（LoRA）是适配器的替代方案， 目前已成为参数高效微调（PEFT）中被广泛使用且效果显著的技术。与适配器方案类似，LoRA 只需更新一小部分参数。</p>
<p>如下图所示，LoRA 的做法不是向模型中添加新的层，而是<strong>在基础模型中创建一个小的子集用于微调</strong>，从而实现高效训练。</p>
<p><img alt="lora" src="../_images/lora.png" /></p>
<p>和适配器一样，LoRA 只需更新基础模型中的一小部分参数，因此可以实现更快速的微调。它的核心思想是：<strong>用较小的矩阵来近似原始大型语言模型中的大矩阵</strong>，从而构建出一个可用于微调的参数子集。</p>
<p>我们可以将这些小矩阵替代原有的大矩阵，并仅对它们进行微调。例如，假设原始模型中有一个 10 × 10 的矩阵</p>
<p><img alt="weight-matrix" src="../_images/weight-matrix.png" /></p>
<p>我们可以用两个较小的矩阵来近似重构这个大矩阵。这两个小矩阵相乘后可以还原成一个 10 × 10 的矩阵。</p>
<p>这带来了显著的效率提升——原来需要使用 <strong>100 个参数</strong>（10×10），现在只需 <strong>20 个参数</strong>（10 + 10）。</p>
<p><img alt="low-rank-matrix" src="../_images/low-rank-matrix.png" /></p>
<p>在训练过程中，我们只需更新这些较小的矩阵，而不需要对整个权重矩阵进行修改。训练后，更新后的变化矩阵（即这些小矩阵）会与原始的（<strong>冻结的</strong>）完整权重一起结合使用，如图 下图 所示。</p>
<p><img alt="full-vs-lora" src="../_images/full-vs-lora.png" /></p>
<p>你可能会担心，这样做会导致模型性能下降，而且确实可能会这样。那么，这种<strong>权衡</strong>在哪些情况下是合理的呢？</p>
<p><a class="reference external" href="https://arxiv.org/abs/2012.13255">Intrinsic dimensionality explains the effectiveness of language model fine-tuning</a> 这篇论文指出，语言模型具有<strong>非常低的内在维度（intrinsic dimensionality）</strong>。这意味着，即使是大型语言模型中的巨大矩阵，也可以通过较小秩（rank）的方式来近似表示。</p>
<p>例如，拥有 1750 亿参数的模型（如 GPT-3），有96 个 Transformer 模块，每个模块都包含一个大小为 12,288 × 12,288 的权重矩阵。这意味着每个模块有多达 <strong>1.5 亿个参数</strong>，但如果我们能够将这个矩阵成功地降秩为 <strong>rank 8</strong>，只需使用两个大小为 12,288 × 2 的矩阵进行替代，这样每个模块只需<strong>约 19.7 万个参数</strong>。这在<strong>速度、存储和计算资源</strong>方面都是极大的节省，这一点在前面提到的 LoRA 论文中有详细解释。</p>
<p>此外，这种较小的表示方式还非常灵活：你可以选择<strong>只微调基础模型的某些部分</strong>。例如，我们可以只微调每个 Transformer 层中的 <strong>Query（查询）和 Value（值）</strong> 权重矩阵。</p>
<section id="id5">
<h3>压缩模型以实现（更）高效的训练<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p>为了使训练更高效，我们可以在将原始权重投影到较小矩阵之前，<strong>先对模型的权重进行压缩</strong>，从而进一步降低内存需求。这种做法可以让 LoRA 的效率更进一步提升。</p>
<p>大型语言模型的权重本质上是带有一定精度的数值，其精度通常以位数表示，例如 <code class="docutils literal notranslate"><span class="pre">float64</span></code>（64位浮点数）或 <code class="docutils literal notranslate"><span class="pre">float32</span></code>（32位浮点数）。如下图，<strong>如果我们减少用于表示一个数值的位数，结果的精度会降低</strong>，但与此同时，模型所需的内存也会显著减少。</p>
<p><img alt="quant" src="../_images/quant.png" /></p>
<p>简而言之，<strong>降低数值精度虽然会牺牲一定的准确性，但能换来更低的内存消耗</strong>，这对于大规模模型训练来说是一种非常实用的优化策略。</p>
<p>量化的目标是在尽量准确地表示原始权重值的同时降低位数。然而，如下图所示，当直接将高精度值映射到低精度值时，多个高精度值可能最终会被相同的低精度值所表示。</p>
<p><img alt="reconstructed-weights" src="../_images/reconstructed-weights.png" /></p>
<p>QLoRA（LoRA 的量化版本）的作者们找到了一种能够在较高位数与较低位数之间互相转换的方法，同时尽量不偏离原始权重。他们采用了块状量化，将某些高精度数值块映射到低精度数值。与直接将高精度映射到低精度不同，他们创建了额外的块，从而对相似的权重进行量化。如下图所示，这使得数值能够在低精度下被准确表示。</p>
<p><img alt="qlora" src="../_images/qlora.png" /></p>
<p>神经网络一个优良的特性是，它们的数值通常在 -1 到 1 之间呈正态分布。这一特性使得我们可以根据权重的相对密度，将原始权重划分到较低位数的区间中，如下图所示。这种映射方式考虑了权重的相对频率，因此更加高效，同时也减少了异常值问题。</p>
<p><img alt="normal-dist" src="../_images/normal-dist.png" /></p>
<p>结合块状量化，归一化过程使得可以用低精度值准确地表示高精度值，同时对大语言模型（LLM）的性能影响极小。因此，我们可以将表示从 16 位浮点数转变为 4 位归一化浮点表示。4 位表示显著降低了 LLM 在训练过程中的内存需求。需要注意的是，总体上对 LLM 进行量化对于推理同样有利，因为经过量化的 LLM 尺寸更小，从而所需显存（VRAM）也更少。</p>
<blockquote>
<div><p>关于量化的完整且具有高度可视化的指南，请参阅这篇博客文章：<a class="reference external" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization">A Visual Guide to Quantization</a>。</p>
</div></blockquote>
</section>
</section>
<section id="rlhf">
<h2>偏好调优 / 对齐 / 基于强化学习的人类反馈（RLHF）<a class="headerlink" href="#rlhf" title="Link to this heading">#</a></h2>
<p>尽管模型现在已经能够遵循指令，但我们还可以通过最后的训练阶段进一步提升其性能，使其符合我们在不同场景下的预期。例如，当被问及“什么是大语言模型（LLM）？”时，我们可能更倾向于获得一个详细描述 LLM 内部机制的答案，而不是仅仅回答“它是一个大语言模型”且没有进一步解释。那么，我们该如何将（人类）对某个答案的偏好与 LLM 的输出进行对齐呢？</p>
<p>我们可以请用户（偏好评估者）来评估该模型生成内容的质量。假设他们给它打了一个分数，比如 4分。模型会基于该分数所代表的偏好进行调优：</p>
<ul class="simple">
<li><p>如果分数较高，模型会被更新，从而鼓励生成更多类似这种类型的内容。</p></li>
<li><p>如果分数较低，模型则会被更新以抑制生成此类内容。</p></li>
</ul>
<p><img alt="preference" src="../_images/preference.png" /></p>
<p>通常我们需要大量训练样本，那么我们能否自动化偏好评估呢？答案是可以的，我们可以通过训练另一种被称为奖励模型的模型来实现这一目标。</p>
<section id="id6">
<h3>使用奖励模型自动化偏好评估<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p>为了实现偏好评估的自动化，在执行偏好调优步骤之前，我们需要先训练一个奖励模型.为了创建一个奖励模型，我们复制经过指令调优的模型，并对其进行微调，使其不再生成文本，而是输出一个单一的分数。</p>
<p><img alt="reward-model" src="../_images/reward-model.png" /></p>
</section>
<section id="id7">
<h3>奖励模型的输入与输出<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p>奖励模型期望的工作方式是：给定一个提示和生成的文本，它会输出一个单一数字，该数字表示该生成在响应该提示时的偏好/质量。</p>
<p><img alt="reward-model-score" src="../_images/reward-model-score.png" /></p>
</section>
<section id="id8">
<h3>训练奖励模型<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<p>我们不能直接使用奖励模型，而是需要先对其进行训练，使其能够正确地为生成结果打分。因此，我们需要一个奖励模型可以学习的偏好数据集。</p>
<section id="id9">
<h4>奖励模型训练数据集<a class="headerlink" href="#id9" title="Link to this heading">#</a></h4>
<p>偏好数据集的一种常见形式是：每个训练样本包含一个提示，以及一个被接受的生成结果和一个被拒绝的生成结果。（注意：这并不总是意味着一个生成结果绝对好，而另一个绝对差；有时两个生成结果都不错，只是其中一个相对更优。）下图展示了包含两个训练样例的偏好训练集。</p>
<p><img alt="rl-examples" src="../_images/rl-examples.png" /></p>
<p>生成偏好数据的一种方法是，向大语言模型（LLM）提供一个提示，并让它生成两个不同的回答，然后我们可以请求人工标注者选择他们更倾向于哪一个回答。</p>
<p><img alt="score" src="../_images/score.png" /></p>
</section>
<section id="id10">
<h4>奖励模型训练步骤<a class="headerlink" href="#id10" title="Link to this heading">#</a></h4>
<p>现在我们已经拥有了偏好训练数据集，可以开始训练奖励模型。一个简单的方法是使用奖励模型对生成结果进行评分：</p>
<ol class="arabic simple">
<li><p>对被接受的生成结果打分</p></li>
<li><p>对被拒绝的生成结果打分</p></li>
</ol>
<p>训练目标是确保被接受的生成结果具有比被拒绝的生成结果更高的得分。</p>
<p><img alt="train-rl" src="../_images/train-rl.png" /></p>
<p>偏好调优的三个阶段：</p>
<ol class="arabic simple">
<li><p>收集偏好数据</p></li>
<li><p>训练奖励模型</p></li>
<li><p>使用奖励模型对大语言模型进行微调（其充当偏好评估者）</p></li>
</ol>
<p><img alt="train-rl-process" src="../_images/train-rl-process.png" /></p>
<p>奖励模型是一个极好的想法，具有进一步扩展和发展的潜力。例如，Llama 2 训练了两个奖励模型：一个用于评估“有帮助程度”，另一个用于评估“安全性”。</p>
<p><img alt="llama" src="../_images/llama.png" /></p>
<p>一种常用的方法是<a class="reference external" href="https://arxiv.org/abs/1707.06347">使用近端策略优化（PPO）</a>来利用训练后的奖励模型对大语言模型进行微调。PPO 是一种流行的强化学习技术，它通过确保大语言模型的输出不会与预期奖励偏差过大，从而对经过指令调优的模型进行优化。事实上，它甚至被用于训练于2022年11月发布的原始版本 ChatGPT。</p>
</section>
<section id="id11">
<h4>训练不使用奖励模型<a class="headerlink" href="#id11" title="Link to this heading">#</a></h4>
<p>PPO（近端策略优化）的一个缺点是，它是一种复杂的方法，至少需要训练两个模型：奖励模型和大语言模型，这可能比实际所需成本更高。</p>
<p>直接偏好优化（DPO）是 PPO 的另一种替代方案，它摒弃了基于强化学习的训练过程，不再使用奖励模型来判断生成结果的质量，而是让大语言模型自身进行判断。如下图所示，我们使用一份大语言模型的拷贝作为参考模型，以比较参考模型与可训练模型在被接受生成结果和被拒绝生成结果质量之间的差距。</p>
<p><img alt="no-rl" src="../_images/no-rl.png" /></p>
<p>通过在训练过程中计算这种偏移，我们可以通过跟踪参考模型和可训练模型之间的差异，从而优化被接受生成结果相较于被拒绝生成结果的可能性。为计算这一偏移及其相关分数，我们分别从参考模型和可训练模型中提取被拒绝生成结果和被接受生成结果的对数概率。如下图所示，这一过程是在 token 级别上执行的，各个 token 的概率被合并起来，用于计算参考模型与可训练模型之间的偏移。</p>
<p><img alt="shift-in-rejecting-scores" src="../_images/shift-in-rejecting-scores.png" /></p>
<p>利用这些分数，我们可以优化可训练模型的参数，使其在生成被接受内容时更加自信，而在生成被拒绝内容时自信度降低。与 PPO 相比，作者们发现 DPO 在训练过程中更加稳定且更准确。鉴于其稳定性，我们将采用 DPO 作为主要模型，对之前经过指令调优的模型进行偏好调优。</p>
</section>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../6-Prompts/jail-break.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">上一页</p>
        <p class="prev-next-title">提示词越狱</p>
      </div>
    </a>
    <a class="right-next"
       href="ft-rep-models.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">下一页</p>
        <p class="prev-next-title">微调表示型大模型</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> 目录
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">预训练</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sft">有监督微调（SFT）</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">偏好微调</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#peft">参数高效微调（PEFT）</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adapters">Adapters（适配器）</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4"></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lora">低秩适配（LoRA）</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">压缩模型以实现（更）高效的训练</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rlhf">偏好调优 / 对齐 / 基于强化学习的人类反馈（RLHF）</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">使用奖励模型自动化偏好评估</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">奖励模型的输入与输出</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">训练奖励模型</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">奖励模型训练数据集</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">奖励模型训练步骤</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">训练不使用奖励模型</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
作者： 高志军
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright Zhijun Gao.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>