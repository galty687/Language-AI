# ä½¿ç”¨MLMå¾®è°ƒBertæ¨¡å‹

ä¸‹æ–¹ä»£ç å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ Hugging Face Transformers åº“å¯¹ Bert æ¨¡å‹è¿›è¡Œ MLM å¾®è°ƒï¼Œå¹¶ä¸”æ¼”ç¤ºäº†å¦‚ä½•å…‹éš†å¹¶å¤„ç† MicrosoftDocs/azure-docs ä»“åº“ä¸­çš„æ–‡æ¡£æ•°æ®ä½œä¸ºè®­ç»ƒæ•°æ®ã€‚



Google Colab Notebook åœ°å€ï¼š [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1HAnmGSR-tCeLtfRrFaCFzpD65ABWJY2n?usp=sharing)

------

### Cell 1: å®‰è£…ä¾èµ–

```python
!pip install transformers datasets
```

------

### Cell 2: å…‹éš† MicrosoftDocs/azure-docs ä»“åº“

è¿™ä¸€æ­¥è€—æ—¶éå¸¸é•¿ï¼Œæœ¬æ¬¡æˆ‘çš„è€—æ—¶çº¦30åˆ†é’Ÿã€‚ä¸ºäº†åç»­æ¼”ç¤ºæ–¹ä¾¿ï¼Œéšæœºåˆ å»äº†å¾ˆå¤šå†…å®¹ï¼Œæœ€ååªä¿ç•™äº†éƒ¨åˆ†æ–‡ä»¶ã€‚

```python
# å…‹éš†ä»“åº“
!git clone https://github.com/MicrosoftDocs/azure-docs.git

# å¯é€‰ï¼šåˆ—å‡ºéƒ¨åˆ† Markdown æ–‡ä»¶ï¼Œç¡®è®¤ä»“åº“å·²å…‹éš†æˆåŠŸ
import glob
md_files = glob.glob("azure-docs/**/*.md", recursive=True)
print(f"å…±æ‰¾åˆ° {len(md_files)} ä¸ª Markdown æ–‡ä»¶ã€‚")
```
è¾“å‡ºç»“æœï¼š

```
Cloning into 'azure-docs'...
remote: Enumerating objects: 7888510, done.
remote: Counting objects: 100% (3279/3279), done.
remote: Compressing objects: 100% (508/508), done.
remote: Total 7888510 (delta 2924), reused 2986 (delta 2771), pack-reused 7885231 (from 3)
Receiving objects: 100% (7888510/7888510), 24.30 GiB | 30.30 MiB/s, done.
Resolving deltas: 100% (6123467/6123467), done.
Updating files: 100% (56225/56225), done.
Found 17449 markdown files.
```



------

### Cell 3: å¤„ç† Markdown æ–‡ä»¶

å°†æ‰€æœ‰markdownæ–‡ä»¶æ‹¼æ¥åˆ—è¡¨æ•°æ®ã€‚

```python
# è¯»å–æ‰€æœ‰ Markdown æ–‡ä»¶ï¼Œå¹¶æ”¶é›†æ–‡æœ¬ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ¸…æ´—æ­¥éª¤ï¼‰
docs_texts = []
for file in md_files:
    try:
        with open(file, "r", encoding="utf-8") as f:
            text = f.read()
            # å¯é€‰ï¼šè¿‡æ»¤æ‰å­—ç¬¦æ•°è¾ƒå°‘çš„æ–‡ä»¶ï¼Œåªä½¿ç”¨è¶…è¿‡200å­—ç¬¦çš„æ–‡ä»¶
            if len(text) > 200:
                docs_texts.append(text)
    except Exception as e:
        print(f"è¯»å– {file} æ—¶å‡ºé”™ï¼š{e}")

print(f"ä»ä»“åº“ä¸­å…±æ”¶é›†åˆ° {len(docs_texts)} ä¸ªæ–‡æ¡£ã€‚")
```

```
ä»ä»“åº“ä¸­å…±æ”¶é›†åˆ° 1621 ä¸ªæ–‡æ¡£ã€‚
```



------



### Cell 4: ä»å¤„ç†åçš„æ•°æ®åˆ›å»º Hugging Face æ•°æ®é›†

```python
from datasets import Dataset

# åˆ›å»ºä¸€ä¸ªåŒ…å«å•ä¸ªå­—æ®µ "text" çš„æ•°æ®é›†
docs_dataset = Dataset.from_dict({"text": docs_texts})
print(docs_dataset)
```

è¾“å‡ºï¼š

```
Dataset({
    features: ['text'],
    num_rows: 1621
})
```



------

### Cell 5: åŠ è½½ bert æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œå¹¶å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯

```python
from transformers import AutoTokenizer, BertForMaskedLM

# Define the checkpoint string
CHECKPOINT = "google-bert/bert-base-uncased"

# Load tokenizer and model from the checkpoint
tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT, trust_remote_code=True)
model = BertForMaskedLM.from_pretrained(CHECKPOINT, trust_remote_code=True)

# Define the tokenization function (adjust max_length as needed)
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=128)

# Assuming docs_dataset is already defined, map the tokenization function
tokenized_docs = docs_dataset.map(tokenize_function, batched=True, remove_columns=["text"])

```

è¾“å‡ºï¼š

```
/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: 
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
tokenizer_config.json:â€‡100%
â€‡48.0/48.0â€‡[00:00<00:00,â€‡5.35kB/s]
config.json:â€‡100%
â€‡570/570â€‡[00:00<00:00,â€‡63.4kB/s]
vocab.txt:â€‡100%
â€‡232k/232kâ€‡[00:00<00:00,â€‡6.17MB/s]
tokenizer.json:â€‡100%
â€‡466k/466kâ€‡[00:00<00:00,â€‡10.4MB/s]
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
model.safetensors:â€‡100%
â€‡440M/440Mâ€‡[00:02<00:00,â€‡184MB/s]
BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Map:â€‡100%
â€‡1621/1621â€‡[00:02<00:00,â€‡587.88â€‡examples/s]
```



------

### Cell 6: å‡†å¤‡ç”¨äº MLM çš„æ•°æ®æ”¶é›†å™¨

```python
from transformers import DataCollatorForLanguageModeling

# åˆ›å»º MLM æ•°æ®æ”¶é›†å™¨ï¼Œè®¾ç½® 15% çš„ mask æ¦‚ç‡
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```

------

### Cell 7: è®¾ç½®è®­ç»ƒå‚æ•°å¹¶åˆå§‹åŒ– Trainer

```python
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./bert-azure-docs-mlm",       # è¾“å‡ºç›®å½•
    overwrite_output_dir=True,
    evaluation_strategy="no",                  # å¦‚éœ€å®šæœŸè¯„ä¼°ï¼Œå¯æ”¹ä¸º "steps"
    num_train_epochs=3,                        # æ ¹æ®éœ€è¦è°ƒæ•´è®­ç»ƒè½®æ¬¡
    per_device_train_batch_size=16,            # æ ¹æ® GPU å®¹é‡è°ƒæ•´ batch size
    save_steps=500,                            # æ¯500æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹
    save_total_limit=2,                        # æœ€å¤šä¿å­˜æœ€è¿‘2ä¸ª checkpoint
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_docs,
)
```

------

### Cell 8: ä½¿ç”¨ MLM ç›®æ ‡å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒ

```python
# å¯¹å¤„ç†åçš„ Azure æ–‡æ¡£æ•°æ®è¿›è¡Œ MLM å¾®è°ƒ
trainer.train()
```

> éœ€è¦wandb.aiçš„key

![wandb-key](images/wandb-key.png)

è¾“å‡ºï¼š

```md
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)
wandb: You can find your API key in your browser here: https://wandb.ai/authorize
wandb: Paste an API key from your profile and hit enter: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·
wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.
wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.
wandb: No netrc file found, creating one.
wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc
wandb: Currently logged in as: galty687 (galty687-peking-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Tracking run with wandb version 0.19.8
Run data is saved locally in /content/wandb/run-20250319_085505-fwlzb07r
Syncing run ./phi4-azure-docs-mlm to Weights & Biases (docs)
View project at https://wandb.ai/galty687-peking-university/huggingface
View run at https://wandb.ai/galty687-peking-university/huggingface/runs/fwlzb07r
 [306/306 00:35, Epoch 3/3]
Step	Training Loss
TrainOutput(global_step=306, training_loss=1.0518618315653083, metrics={'train_runtime': 61.0276, 'train_samples_per_second': 79.685, 'train_steps_per_second': 5.014, 'total_flos': 319991251161600.0, 'train_loss': 1.0518618315653083, 'epoch': 3.0})
```



------

### Cell 9: ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹å’Œåˆ†è¯å™¨

```python
from google.colab import drive
import os

# æŒ‚è½½ Google Drive
drive.mount('/content/drive')

# å®šä¹‰ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨çš„ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆfine-tuned-models æ–‡ä»¶å¤¹ä¸­ï¼‰
dest_folder = '/content/drive/MyDrive/fine-tuned-models/bert-azure-docs-mlm'
os.makedirs(dest_folder, exist_ok=True)

# ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹å’Œåˆ†è¯å™¨åˆ°ç›®æ ‡è·¯å¾„
model.save_pretrained(dest_folder)
tokenizer.save_pretrained(dest_folder)

print(f"æ¨¡å‹å’Œåˆ†è¯å™¨å·²ä¿å­˜åˆ° {dest_folder} æ–‡ä»¶å¤¹ä¸­ã€‚")
```

------

è¾“å‡ºï¼š

```
Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).
æ¨¡å‹å’Œåˆ†è¯å™¨å·²ä¿å­˜åˆ° /content/drive/MyDrive/fine-tuned-models/bert-azure-docs-mlm æ–‡ä»¶å¤¹ä¸­ã€‚
```

![stored-model](images/stored-model.png)

ä»¥ä¸Šä»£ç åˆ†ä¸ºå¤šä¸ªå•å…ƒæ ¼ï¼Œä½ å¯ä»¥åœ¨ Google Colab ä¸­ä¾æ¬¡è¿è¡Œã€‚ä½ å¯ä»¥æ ¹æ®å®é™…æ•°æ®å’Œ GPU èµ„æºï¼Œè°ƒæ•´ `max_length`ã€`num_train_epochs`ã€batch size ç­‰å‚æ•°ã€‚æ­¤æµç¨‹å…ˆåˆ©ç”¨ MLM ç›®æ ‡å¾®è°ƒæ¨¡å‹ï¼Œå†åç»­å¯ä»¥æ ¹æ®éœ€è¦ç”¨å¸¦æœ‰ä¸åŒé£æ ¼çš„ SFT æ•°æ®è¿›ä¸€æ­¥è¿›è¡Œå¾®è°ƒã€‚



---

å®éªŒè¿‡ç¨‹ä¸­ï¼Œæœ‰æ—¶å€™å› ä¸ºå„ç§åŸå› ä¼šé‡å¯sessionï¼Œå¯ä»¥è®²å¾®è°ƒæ•°æ®å­˜å…¥Google Driveï¼ŒèŠ‚çº¦é‡å¤ä¸‹è½½çš„æ—¶é—´ã€‚



### å‡†å¤‡æ•°æ®

å°†æ‰€æœ‰markdownæ–‡ä»¶è¯»å–åï¼Œæ‹¼æ¥ä¸ºä¸€ä¸ªå•ç‹¬çš„ `docs_texts`åˆ—è¡¨ã€‚ 

```python
from google.colab import drive
import os, glob

# æŒ‚è½½ Google Drive
drive.mount('/content/drive')

# å®šä¹‰ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„
dest_folder = '/content/drive/MyDrive/pure-md-files'

# ä½¿ç”¨ glob è·å–ç›®æ ‡æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰ Markdown æ–‡ä»¶ï¼ˆæ ¹ç›®å½•ä¸‹çš„ï¼‰
md_files = glob.glob(os.path.join(dest_folder, '*.md'))
docs_texts = []

for file in md_files:
    try:
        with open(file, "r", encoding="utf-8") as f:
            text = f.read()
            # è¿‡æ»¤æ‰å­—ç¬¦æ•°è¾ƒå°‘çš„æ–‡ä»¶ï¼Œåªä½¿ç”¨è¶…è¿‡200å­—ç¬¦çš„æ–‡ä»¶
            if len(text) > 200:
                docs_texts.append(text)
    except Exception as e:
        print(f"è¯»å– {file} æ—¶å‡ºé”™ï¼š{e}")

print(f"ä»ä»“åº“ä¸­å…±æ”¶é›†åˆ° {len(docs_texts)} ä¸ªæ–‡æ¡£ã€‚")
```

### å°†æ•°æ®è½¬ä¸ºjsonå­˜å…¥Google Drive

```python
import os
import json

# å®šä¹‰ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå·²æŒ‚è½½åˆ° Google Driveï¼‰
dest_folder = '/content/drive/MyDrive/pure-md-files'

# æ„é€ ä¿å­˜æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
save_path = os.path.join(dest_folder, 'docs_texts.json')

# ä¿å­˜ docs_texts åˆ°ç›®æ ‡è·¯å¾„ä¸­çš„ JSON æ–‡ä»¶
with open(save_path, 'w', encoding='utf-8') as f:
    json.dump(docs_texts, f, ensure_ascii=False, indent=2)

print(f"docs_texts å·²ä¿å­˜åˆ° {save_path} æ–‡ä»¶ã€‚")

```

### ä»ç›®æ ‡è·¯å¾„ä¸­çš„ JSON æ–‡ä»¶ä¸­åŠ è½½ docs_texts

```python
from google.colab import drive
import os
import json

# æŒ‚è½½ Google Drive
drive.mount('/content/drive')

# å®šä¹‰ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå·²æŒ‚è½½åˆ° Google Driveï¼‰
dest_folder = '/content/drive/MyDrive/pure-md-files'
# æ„é€ æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
load_path = os.path.join(dest_folder, 'docs_texts.json')

# ä»ç›®æ ‡è·¯å¾„ä¸­çš„ JSON æ–‡ä»¶ä¸­åŠ è½½ docs_texts
with open(load_path, 'r', encoding='utf-8') as f:
    docs_texts = json.load(f)

print(f"docs_texts å·²ä» {load_path} æ–‡ä»¶ä¸­åŠ è½½ã€‚")

```





