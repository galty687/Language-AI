# 术语

**人工智能（AI, Artificial Intelligence）**

指通过计算机系统来模拟或延伸人类智能行为的技术和理论的集合。它包含机器学习、深度学习、自然语言处理、计算机视觉等众多子领域。



**机器学习（Machine Learning）**

让计算机能够从数据中自动学习和改进的一类方法论，其核心目标是通过样本数据训练得到模型并进行预测或决策。



**神经网络（Neural Network）**

人工构建的类似于生物神经元结构的网络。通过对大量加权求和与非线性激活函数的组合来对数据进行处理、学习和预测。



**监督学习（Supervised Learning）**

通过已有的输入-输出（特征-标签）配对数据来训练模型，使模型能够对新数据进行预测。例如：图像分类、语音识别等。



**强化学习（Reinforcement Learning）**

通过智能体（Agent）与环境交互获得奖励（Reward）或惩罚（Penalty）进行学习，优化决策策略（Policy）。常见应用包括游戏AI、机器人控制等。



**无监督学习（Unsupervised Learning）**

数据没有明确的标签，模型通过挖掘数据内部的结构（如聚类、降维等）来发现潜在模式。例如：K-Means聚类、主成分分析（PCA）等。



**半监督学习（Semi-Supervised Learning）**

部分数据带有标签，部分数据无标签，通过利用大量无标签数据的分布特性来帮助模型学习，提高模型的准确率。



**强化学习（Reinforcement Learning）**

通过智能体（Agent）与环境交互获得奖励（Reward）或惩罚（Penalty）进行学习，优化决策策略（Policy）。常见应用包括游戏AI、机器人控制等。



**元学习（Meta-Learning）**

又称“学习如何学习”，模型通过在不同任务上的学习经验来加速对新任务的学习，提升泛化能力。



**在线学习（Online Learning）**

模型在不断获取新数据流的过程中持续更新，通过“边学习边预测”的方式应对动态环境。



---

**感知器（Perceptron）**

最简单的神经网络结构之一，仅包含输入层和输出层，用于线性可分问题的分类。



**卷积神经网络（CNN, Convolutional Neural Network）**

利用卷积层和池化层来抽取局部特征，广泛应用于图像处理、视频分析等领域。



**循环神经网络（RNN, Recurrent Neural Network）**

专门处理序列数据（如文本、语音等）的网络结构，通过在隐藏层中保留“记忆”来处理上下文。



**长短期记忆（LSTM, Long Short-Term Memory）**

解决RNN中“梯度消失/梯度爆炸”问题的特殊结构，能够在较长序列中保留并传递重要信息。



**门控循环单元（GRU, Gated Recurrent Unit）**

是LSTM的简化版本，具有类似的效果，但参数更少，计算效率更高。



**Transformer**

摒弃循环结构，完全依赖“注意力机制（Attention Mechanism）”来捕捉序列依赖的网络结构，大幅提升并行计算效率，也是大多数大型语言模型（LLM）的基础。

---

**大型语言模型（LLM, Large Language Model）**

指在大规模文本数据上训练得到的、拥有数亿到数千亿参数的语言模型（如GPT、BERT 等），具备强大的自然语言理解和生成能力。



**GPT（Generative Pre-trained Transformer）**

一系列由OpenAI提出的生成式预训练模型，基于Transformer结构，能够进行强大的文本生成与理解。



**BERT（Bidirectional Encoder Representations from Transformers）**

由Google提出的双向编码器预训练模型，擅长理解句子上下文和词义，多用于自然语言理解、文本分类等任务。



**SFT（Supervised Fine-Tuning，监督微调）**

在训练好的预训练模型之上，利用带标签的数据对模型进行进一步的“微调”，使模型在特定任务上具有更强的表现。例如，针对对话任务或特定领域文本的二次训练。

 

**RLHF（Reinforcement Learning from Human Feedback）**

结合人类反馈（如人工标注的偏好）和强化学习，将人类偏好纳入到模型训练目标中，从而得到更符合人类期望或偏好的模型回复。



**自监督学习（Self-Supervised Learning）**

不依赖人工标注数据，而是从数据自身构造出训练信号（如掩码预测、下一词预测等），常见于大型语言模型的预训练阶段。



**提示工程（Prompt Engineering）**

在使用大型语言模型时，通过精心设计Prompt（提示语句）来引导模型生成更准确或更符合需求的结果。

---

**迁移学习（Transfer Learning）**

将在某一任务或领域学到的知识迁移到另外相似任务或领域，常见方式是用预训练模型做特征提取或进行微调（Fine-Tuning）。



**多任务学习（Multi-Task Learning）**

同时学习多个相关任务，以达到共享知识、提高总体模型性能的目的。



**数据增强（Data Augmentation）**

在图像、文本等领域，通过对训练样本进行变换（如旋转、翻转、添加噪声）来增加数据量，缓解过拟合问题。



**正则化（Regularization）**

在模型训练时添加额外约束或惩罚（L1、L2、Dropout等），以防止模型过拟合、提升泛化能力。



**超参数调整（Hyperparameter Tuning）**

调整学习率、批大小、网络深度等超参数，找出能使模型达到最佳性能的组合。



**过拟合（Overfitting）**

机器学习和深度学习中常见的问题，指的是模型在训练数据上表现非常好，但在新数据（验证集或测试集）上表现较差的现象。它主要是因为模型“记住”了训练集中的噪声或与真实任务无关的细节，导致泛化能力下降。



**早停（Early Stopping）**

在验证集性能不再提升时提前结束训练，避免模型过度拟合训练集。



---

**准确率（Accuracy）**

预测正确的样本数与总样本数之比，适合于数据集分布相对平衡的场景。

**精确率（Precision）**

在预测为正类的样本中，真正为正类的占比。

**召回率（Recall）**

在真实为正类的样本中，被正确预测为正类的占比。

**F1分数（F1 Score）**

精确率与召回率的调和平均，综合度量模型精确率和召回率。

**ROC 曲线（ROC Curve）和 AUC（Area Under Curve）**

衡量分类器在不同阈值下的表现，AUC表示ROC曲线下的面积，AUC越大代表分类器整体性能越好。

**混淆矩阵（Confusion Matrix）**

以矩阵形式展示分类任务中实际标签与预测标签的对比情况。



---

**损失函数（Loss Function）**

度量模型预测与真实值之间差异的函数，如均方误差（MSE）、交叉熵损失（Cross-Entropy）等。



**梯度下降（Gradient Descent）**

通过计算损失函数对模型参数的梯度来更新参数的优化方法，常见批量梯度下降、小批量梯度下降等。



**反向传播（Backpropagation）**

通过计算损失函数对每个参数的偏导数来更新参数，是神经网络训练的核心算法之一。



**降维（Dimensionality Reduction）**

将高维数据映射到低维空间的过程，常见算法有PCA（主成分分析）、t-SNE 等。



**聚类（Clustering）**

将相似度较高的样本划分到同一组的方法，如K-Means、DBSCAN等。



---

**全量微调（Full Fine-Tuning）**

指在预训练模型的基础上，对模型中所有参数进行端到端训练的方式。优点是能够充分利用模型的强大表达能力，缺点是训练和部署的开销较大，且容易在小数据集上过拟合。

**指令微调（Instruction Tuning）**

通过在训练数据中加入大量“指令-回复”示例，对模型进行微调，使其更好地理解和执行人类以自然语言形式给出的指令。在对话系统、问答系统等场景中广泛应用。

**Adapter Tuning（Adapter Tuning）**

在预训练模型的某些层插入轻量级的 Adapter 模块，微调时仅更新这些新增的 Adapter 参数，主干网络保持冻结。

**Prefix Tuning / P-Tuning（Prefix Tuning / P-Tuning）**在 Transformer 的输入序列中添加可学习的“前缀”，并仅对这部分前缀参数进行训练，引导预训练模型关注目标任务。

**LoRA（LoRA, Low-Rank Adaptation）**

对需要更新的权重矩阵进行低秩分解，只训练分解后的两个小矩阵，从而大幅减少需更新的参数量。

**BitFit（BitFit）**

只微调模型中的偏置（bias）参数，冻结所有权重矩阵，进一步降低训练成本，但在复杂任务上的效果较有限。
