# 提示词越狱

通过精心构造的Prompt是可以让大模型进行越狱的，越狱追溯于早期 IOS，用户为了突破设备的封闭生态系统，自由操作自己的IOS，不被限制，而在大模型中，越狱同理，规避大模型的限制，执行那些被禁止的行为，在我理解里面，越狱关键在于打破常规，绕过限制以获得常规之外的权限，而Prompt越狱的玩法多样，技巧繁多，操控输入Prompt，以误导或恶意操纵AI模型的攻击方式其核心在于利用AI对文本指令的高度依赖性，通过巧妙设计的输入，使AI输出意料之外甚至是有害的内容。



## 角色扮演，温柔乡

Windows序列号获取



## 反向诱导，逆向思维





## PUA道德绑架





## 小语种





## 参考：



1. https://github.com/Acmesec/PromptJailbreakManual
2. 