# 微调基础

## 硬件需求

### Qwen-7B 为例

| 项目           | Qwen-7B 推荐配置                    |
| -------------- | ----------------------------------- |
| **推理**       | 16GB 显卡 + 8GB RAM 即可运行        |
| **LoRA 微调**  | ≥ 24GB 显卡 + 32GB RAM              |
| **QLoRA 微调** | ≥ 12GB 显卡 + 16GB RAM              |
| **全参微调**   | ≥ 48GB 显卡 或多卡分布式            |
| **硬盘需求**   | 模型+日志+checkpoint 合计约 30~50GB |



## 常见概念

### Epoch（训练轮次）

完整地遍历一次训练集的全部数据称为一个 Epoch。 例如：3个 Epoch 表示模型会将整个数据集完整地学习3遍。



### Batch Size（批次大小）

一次模型更新所使用的样本数量。 例如：Batch Size 为 8，表示每次拿8个样本进行训练，然后再更新一次模型的参数。



### Learning Rate（学习率）

模型参数每次更新的步长大小。
例如：学习率越大，模型更新参数的步子迈得越大，训练速度可能快但容易错过最优点；学习率越小，更新步子小，训练过程更加细致稳定，但可能训练缓慢。

一个好的类比：

想象你在雾中下山，目标是到达谷底。学习率就像你每一步迈出的距离：

- 迈得太大（学习率过高），可能一脚踏空，甚至掉到悬崖；
- 迈得太小（学习率过低），虽然安全，却要走很久才能到达谷底。



#### Checkpoint（模型检查点）

模型训练到一定阶段时保存的中间结果，可以在中断或失败时从该点继续训练。 例如：训练到一半（如第2个Epoch后）保存Checkpoint，出现意外中断后能从此处继续训练，不用从头开始。



### 过拟合与欠拟合

![under-over](images2/under-over.jpeg)



## 常用微调方法

| **微调方法**         | **方式**          | **参数调整**   | **计算成本** | **适用场景**   |
| -------------------- | ----------------- | -------------- | ------------ | -------------- |
| **GROP**             | 软提示 + 梯度优化 | 仅优化提示嵌入 | 低           | 高效任务适配   |
| **Full Fine-tuning** | 全参数微调        | 所有模型参数   | 高           | 需要大规模调整 |
| **LoRA**             | 低秩适配          | 仅优化部分矩阵 | 中等         | 适用于大型模型 |
| **Prefix Tuning**    | 软提示            | 仅优化前缀嵌入 | 低           | 生成任务优化   |
| **Adapter Tuning**   | 适配层            | 仅优化额外模块 | 适中         | 可扩展性强     |